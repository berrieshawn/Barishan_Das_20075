Cross Entropy is typically preferred for logistic regression because it is specifically designed for classification tasks,
providing a clear and interpretable measure of error. It encourages the model to output probabilities that match the true labels, making it suitable for logistic regression where the goal is to predict binary outcomes. Additionally, Cross Entropy ensures a single best answer by penalizing deviations from the correct class probabilities.
